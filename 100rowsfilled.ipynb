{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfImALeuMN1ev9Qm65IxUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halfbug/colab/blob/main/100rowsfilled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_yLeh0hiW9Iv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_id = '1M8mauGQDZVbXpwQPAlfrKc2suVqtdB5k'\n",
        "sheet_name = \"LevelGuide\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "df=pd.read_csv(url)\n",
        "# https://docs.google.com/spreadsheets/d/1M8mauGQDZVbXpwQPAlfrKc2suVqtdB5k/edit#gid=816935593"
      ],
      "metadata": {
        "id": "jzAAwhqwYF2I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K85q2qqtYQJC",
        "outputId": "f0337fcd-d475-474e-b0b8-9e365b856712"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  join  Hemster ID  \\\n",
            "0       55718 194900722053 30F1GCDS2L 001 Michael Kors       55718   \n",
            "1      55740 196163322728 30S2G9HM6L 1999 Michael Kors       55740   \n",
            "2    55760 196163134963 30S2SCDT3L chambray Michael...       55760   \n",
            "3          55770 193144528056 MM7W775 110 Michael Kors       55770   \n",
            "4    55858 196163135120 30S2SGRL8L CHAMBRAY Michael...       55858   \n",
            "..                                                 ...         ...   \n",
            "134     55653 196108954069 43T2OLFS1D 005 Michael Kors       55653   \n",
            "135     55629 196237053305 MU2814R5PT 100 Michael Kors       55629   \n",
            "136    55625 196163150185 MS2206R50R 3110 Michael Kors       55625   \n",
            "137     55640 196237063922 MU2817O33D 001 Michael Kors       55640   \n",
            "138     55631 723088612322 77Q5928M42 110 Michael Kors       55631   \n",
            "\n",
            "           MK SKU          Description Grade         Garment Type  \\\n",
            "0    194900722053       30F1GCDS2L 001     a                 Bags   \n",
            "1    196163322728      30S2G9HM6L 1999     a                 Bags   \n",
            "2    196163134963  30S2SCDT3L chambray     a                 Bags   \n",
            "3    193144528056          MM7W775 110     a              Scarves   \n",
            "4    196163135120  30S2SGRL8L CHAMBRAY     a                 Bags   \n",
            "..            ...                  ...   ...                  ...   \n",
            "134  196108954069       43T2OLFS1D 005     a                Shoes   \n",
            "135  196237053305       MU2814R5PT 100     a       Sleeveless Top   \n",
            "136  196163150185      MS2206R50R 3110     a            Outerwear   \n",
            "137  196237063922       MU2817O33D 001     a  Basic Sleeved Dress   \n",
            "138  723088612322       77Q5928M42 110     a            Outerwear   \n",
            "\n",
            "    Category Type     MSRP  brand   image  Sold on amazon  color  \\\n",
            "0       Accessory  $200.00     NaN    NaN             NaN    NaN   \n",
            "1       Accessory  $138.00     NaN    NaN             NaN    NaN   \n",
            "2       Accessory  $150.00     NaN    NaN             NaN    NaN   \n",
            "3       Accessory  $100.00     NaN    NaN             NaN    NaN   \n",
            "4       Accessory  $250.00     NaN    NaN             NaN    NaN   \n",
            "..            ...      ...     ...    ...             ...    ...   \n",
            "134      Footwear  $110.00     NaN    NaN             NaN    NaN   \n",
            "135       Apparel  $150.00     NaN    NaN             NaN    NaN   \n",
            "136       Apparel  $595.00     NaN    NaN             NaN    NaN   \n",
            "137       Apparel  $260.00     NaN    NaN             NaN    NaN   \n",
            "138       Apparel  $220.00     NaN    NaN             NaN    NaN   \n",
            "\n",
            "     Amazon price  ASIN  UPC  \n",
            "0             NaN   NaN  NaN  \n",
            "1             NaN   NaN  NaN  \n",
            "2             NaN   NaN  NaN  \n",
            "3             NaN   NaN  NaN  \n",
            "4             NaN   NaN  NaN  \n",
            "..            ...   ...  ...  \n",
            "134           NaN   NaN  NaN  \n",
            "135           NaN   NaN  NaN  \n",
            "136           NaN   NaN  NaN  \n",
            "137           NaN   NaN  NaN  \n",
            "138           NaN   NaN  NaN  \n",
            "\n",
            "[139 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logger"
      ],
      "metadata": {
        "id": "4TygYEzH7paq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, logger_name):\n",
        "        self.logger = logging.getLogger(logger_name)\n",
        "        self.logger.setLevel(logging.DEBUG)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        ch = logging.StreamHandler()\n",
        "        ch.setFormatter(formatter)\n",
        "        self.logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "HxMn1BfNVt4K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ExcelProcessor**"
      ],
      "metadata": {
        "id": "rgKOubuG7mj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExcelProcessor:\n",
        "    def __init__(self, file_path=None, sheet_id=None, sheet_name=\"Excel file\"):\n",
        "        # Example usage for opening a Google Sheet:\n",
        "        # sheet_id = '1M8mauGQDZVbXpwQPAlfrKc2suVqtdB5k'\n",
        "        # sheet_name = \"LevelGuide\"\n",
        "        # processor = ExcelProcessor(sheet_id=sheet_id, sheet_name=sheet_name)\n",
        "\n",
        "        if file_path:\n",
        "            self.df = pd.read_excel(file_path)\n",
        "        elif sheet_id and sheet_name:\n",
        "            url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "            self.df = pd.read_csv(url)\n",
        "        else:\n",
        "            raise ValueError(\"Either provide a file path or Google Sheet ID and sheet name.\")\n",
        "        self.logger = Logger(__name__).logger\n",
        "\n",
        "    def get_empty_columns_names(self):\n",
        "        empty_columns = self.df.columns[self.df.isnull().any()].tolist()\n",
        "        return empty_columns\n",
        "\n",
        "    def get_filled_columns_names(self):\n",
        "        filled_columns = self.df.columns[self.df.notnull().any()].tolist()\n",
        "        return filled_columns\n",
        "\n",
        "    def list_columns_with_values(self,num_columns=None):\n",
        "        non_empty_columns = self.df.columns[self.df.notnull().any()].tolist()\n",
        "        filtered_df = self.df[non_empty_columns]\n",
        "        if num_columns:\n",
        "            return filtered_df.values[:, :num_columns].tolist()\n",
        "        else:\n",
        "            return filtered_df.values.tolist()\n",
        "        return columns_with_values\n",
        "\n",
        "    def get_all_rows(self):\n",
        "        return self.df.values.tolist()\n",
        "\n",
        "    def get_all_rows_values(self):\n",
        "        non_empty_columns = self.df.columns[self.df.notnull().any()].tolist()\n",
        "        filtered_df = self.df[non_empty_columns]\n",
        "        return filtered_df.values.tolist()\n",
        "\n",
        "    def get_total_rows_columns(self):\n",
        "        total_rows, total_columns = self.df.shape\n",
        "        self.logger.info(\"get total rows and columns in the sheet\")\n",
        "        return total_rows, total_columns\n",
        "        # Example usage to get total rows and columns in the sheet\n",
        "        # total_rows, total_columns = processor.get_total_rows_columns()\n",
        "        # print(\"Total rows:\", total_rows)\n",
        "        # print(\"Total columns:\", total_columns)"
      ],
      "metadata": {
        "id": "bJCLz2UAjiqN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_id = '1M8mauGQDZVbXpwQPAlfrKc2suVqtdB5k'\n",
        "sheet_name = \"LevelGuide\"\n",
        "processor = ExcelProcessor(sheet_id=sheet_id, sheet_name=sheet_name)\n",
        "\n",
        "print(\"Empty columns:\", processor.get_empty_columns_names())\n",
        "\n",
        "# print(\"Columns with values:\", processor.get_filled_columns_names()[ :3])\n",
        "print(\"Columns with values:\", ' '.join(processor.get_filled_columns_names()))\n",
        "\n",
        "print(\"All rows of the sheet:\")\n",
        "all_rows = processor.get_all_rows_values()\n",
        "for row in all_rows:\n",
        "    # print(row)\n",
        "    row_str = \", \".join(map(str, row))\n",
        "    # print(row_str)\n",
        "\n",
        "# Example usage to get total rows and columns in the sheet\n",
        "total_rows, total_columns = processor.get_total_rows_columns()\n",
        "print(\"Total rows:\", total_rows)\n",
        "print(\"Total columns:\", total_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR9oYhMdlTJn",
        "outputId": "93aa447a-a313-4644-82ad-4accb586412f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "2024-03-21 07:07:27,278 - __main__ - INFO - get total rows and columns in the sheet\n",
            "INFO:__main__:get total rows and columns in the sheet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty columns: ['brand ', 'image', 'Sold on amazon', 'color', 'Amazon price', 'ASIN', 'UPC']\n",
            "Columns with values: join Hemster ID MK SKU Description Grade Garment Type Category Type MSRP\n",
            "All rows of the sheet:\n",
            "Total rows: 139\n",
            "Total columns: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for columns with empty values\n",
        "empty_columns = df.columns[df.isnull().any()].tolist()\n",
        "empty_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjYTK9FpbL9s",
        "outputId": "dee597ca-3248-4c44-f6a2-ea5a8695c595"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brand ', 'image', 'Sold on amazon', 'color', 'Amazon price', 'ASIN', 'UPC']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HelperText"
      ],
      "metadata": {
        "id": "SQ7dIU4G7dDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HelperText:\n",
        "    @staticmethod\n",
        "    def format_string(input_string):\n",
        "        formatted_string = \"\"\n",
        "\n",
        "        # Split the input string into separate lines\n",
        "        lines = input_string.split('\\n')\n",
        "\n",
        "        # Iterate through each line\n",
        "        for line in lines:\n",
        "            # Check if the line contains a colon ':'\n",
        "            if ':' in line:\n",
        "                # Split the line into key and value based on the colon ':'\n",
        "                key, value = line.split(':', 1)\n",
        "                # Format the key and value and add them to the formatted string\n",
        "                formatted_string += f\"{key.strip()}: {value.strip()} \"\n",
        "            else:\n",
        "                # If the line does not contain a colon, add it as is to the formatted string\n",
        "                formatted_string += f\"{line.strip()} \"\n",
        "\n",
        "        return formatted_string\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_code(input_string):\n",
        "        string_encode = input_string.encode(\"ascii\", \"ignore\")\n",
        "        input_string = string_encode.decode()\n",
        "        return(input_string)\n",
        "\n",
        "# Example usage:\n",
        "helper = HelperText()\n",
        "\n",
        "# Format a string\n",
        "formatted_string = helper.format_string(\"Your input string\")\n",
        "print(\"Formatted string:\", formatted_string)\n",
        "\n",
        "# Clear code from a string\n",
        "cleaned_string = helper.clear_code(\"Your input string with code\")\n",
        "print(\"Cleaned string:\", cleaned_string)\n"
      ],
      "metadata": {
        "id": "JVrf2YGm3aQp",
        "outputId": "122b3651-2aa1-4f3c-e890-8ff5f45ca3dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted string: Your input string \n",
            "Cleaned string: Your input string with code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WebScraper**"
      ],
      "metadata": {
        "id": "ZG2x1i3j7TL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import userdata\n",
        "\n",
        "class WebScraper:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://www.google.com/search?q=\"\n",
        "        self.api_key = userdata.get('GOOGLE_SEARCH_API')\n",
        "        self.service = build(\"customsearch\", \"v1\", developerKey=self.api_key)\n",
        "        self.logger = Logger(__name__).logger\n",
        "\n",
        "    def google_scraper(self, query):\n",
        "        try:\n",
        "          response = requests.get(self.base_url+query.lower().replace(' ', '+'))\n",
        "          soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "          # find first five link's content\n",
        "          # anchor_tags = soup.find_all('a', href=True)\n",
        "          # hrefs = []\n",
        "          # content = \"\"\n",
        "          # for tag in anchor_tags:\n",
        "          #   href = tag.get('href')\n",
        "          #   if href and href.startswith(\"/url?q=https\") and \"google\" not in href.lower():  # Check if href exists (it might be None)\n",
        "          #     parts = href.split('&', 1)\n",
        "          #     parts = parts[0].split('/url?q=',1)\n",
        "          #     # print(parts)\n",
        "          #     hrefs.append(parts[1])\n",
        "          #     scraped_content = scrape_text(parts[1])\n",
        "          #     content += scraped_content + \"\\n ***\"\n",
        "              # print(scraped_content)\n",
        "          # print(hrefs[:5])\n",
        "\n",
        "\n",
        "\n",
        "          body_content = soup.find('body')  # Find the body tag\n",
        "\n",
        "          if body_content:\n",
        "            # Extract only the text content from the body tag\n",
        "              text_content = body_content.get_text(strip=True)\n",
        "              # print(\"\\033[34m\" + text_content )\n",
        "\n",
        "\n",
        "\n",
        "              return helper.clear_code(text_content)    #format_string(content)\n",
        "        except Exception as e:\n",
        "          print(f\"Error occurred while scraping: {e}\")\n",
        "          return \"No search result found: Unknown\"\n",
        "        self.logger.info(\"Scraping Google search results for: %s\", query)\n",
        "\n",
        "    def scrape_text(self,url):\n",
        "    # Function to scrape the webpage for availability information\n",
        "      try:\n",
        "          response = requests.get(url)\n",
        "          soup = BeautifulSoup(response.content, 'html.parser')\n",
        "          # Search for specific keywords like \"stock availability\" on the webpage\n",
        "\n",
        "          # Step 3: Extract metadata\n",
        "          meta_tags = soup.find_all('meta')\n",
        "          metadata = {}\n",
        "          for tag in meta_tags:\n",
        "              if 'name' in tag.attrs:\n",
        "                  name = tag.attrs['name']\n",
        "                  content = tag.attrs.get('content', '')\n",
        "                  metadata[name] = content\n",
        "              elif 'property' in tag.attrs:  # For OpenGraph metadata\n",
        "                  property = tag.attrs['property']\n",
        "                  content = tag.attrs.get('content', '')\n",
        "                  metadata[property] = content\n",
        "\n",
        "          # Display the metadata\n",
        "          # for key, value in metadata.items():\n",
        "          #     print(f\"{key}: {value}\")\n",
        "\n",
        "          # print( metadata)\n",
        "\n",
        "          meta_tag = self.extract_product_info([metadata])\n",
        "\n",
        "              # Find all link elements with the 'as' attribute set to 'image'\n",
        "          image_links = soup.find_all('link', {'rel': 'preload', 'as': 'image'})\n",
        "\n",
        "          # Extract and print the href attributes\n",
        "          images = ''\n",
        "          for link in image_links:\n",
        "              image_url = link.get('href')\n",
        "              if image_url:\n",
        "                  if image_url.startswith(\"//\"):\n",
        "                    # Add protocol (http assumed):\n",
        "                    image_url= f\"http:{image_url}\"\n",
        "                  images = images +\",\"+ image_url\n",
        "                  # print(image_url)\n",
        "\n",
        "          body_content = soup.find('body')  # Find the body tag\n",
        "          print(body_content)\n",
        "          if body_content:\n",
        "            # Extract only the text content from the body tag\n",
        "              text_content = body_content.get_text(strip=True)\n",
        "\n",
        "              # Search for either \"stock\" or \"stock availability\" in the text content\n",
        "              # print(text_content)\n",
        "              string_encode = text_content.encode(\"ascii\", \"ignore\")\n",
        "              text_content = string_encode.decode()\n",
        "              return text_content + meta_tag + \"Images : \" + images\n",
        "          return f\"Content: Unknown on {url}\"\n",
        "      except Exception as e:\n",
        "          print(f\"Error occurred while scraping: {e}\")\n",
        "          return f\"Content: Unknown on {url}\"\n",
        "\n",
        "    def extract_product_info(self, meta_tags):\n",
        "        # Function to extract product information from meta tags\n",
        "        product_info = \"\"\n",
        "        for meta_tag in meta_tags:\n",
        "            if 'og:title' in meta_tag:\n",
        "                product_info += f\"\\n ***Title: {meta_tag['og:title']}\\n\"\n",
        "            if 'og:description' in meta_tag:\n",
        "                product_info += f\"Description: {meta_tag['og:description']}\\n\"\n",
        "            if 'og:price:amount' in meta_tag and 'og:price:currency' in meta_tag:\n",
        "                product_info += f\"Price: {meta_tag['og:price:amount']} {meta_tag['og:price:currency']}\\n\"\n",
        "            if 'og:image' in meta_tag:\n",
        "                    if len(meta_tag['og:image']) <= 200:  # Check if URL length is less than or equal to 200\n",
        "                        product_info += f\"Image URL: {meta_tag['og:image']} \\n\"\n",
        "            if 'og:url' in meta_tag:\n",
        "                product_info += f\"URL: {meta_tag['og:url']} \\n\"\n",
        "                # Call function to extract availability information\n",
        "                availability_info = self.extract_availability(meta_tag['og:url'])\n",
        "                product_info += f\"{availability_info}\\n\"\n",
        "                # Extract ASIN from URL if it's an Amazon link\n",
        "                asin_match = re.search(r'/dp/(\\w+)', meta_tag['og:url'])\n",
        "                if asin_match:\n",
        "                    product_info += f\"ASIN: {asin_match.group(1)} \\n\"\n",
        "            if 'og:site_name' in meta_tag:\n",
        "                product_info += f\"Site Name: {meta_tag['og:site_name']} \\n\"\n",
        "            if 'og:availability' in meta_tag:\n",
        "                product_info += f\"Availability: {meta_tag['og:availability']} \\n\"\n",
        "            if 'og:type' in meta_tag:\n",
        "                    product_info += f\"type: {meta_tag['og:type']} \\n\"\n",
        "            else:\n",
        "              product_info += f\"type: text \\n\"\n",
        "        return product_info\n",
        "\n",
        "\n",
        "    def extract_availability(self,url):\n",
        "    # Function to scrape the webpage for availability information\n",
        "      try:\n",
        "          response = requests.get(url)\n",
        "          soup = BeautifulSoup(response.content, 'html.parser')\n",
        "          # Search for specific keywords like \"stock availability\" on the webpage\n",
        "\n",
        "          body_content = soup.find('body')  # Find the body tag\n",
        "\n",
        "          if body_content:\n",
        "            # Extract only the text content from the body tag\n",
        "              text_content = body_content.get_text(strip=True)\n",
        "              # Search for either \"stock\" or \"stock availability\" in the text content\n",
        "              # print(text_content)\n",
        "              #  availability_pattern = re.compile(r\"(?:stock|stock|availability|Number\\sof\\sItems\\s*(\\d+)|in\\s*stock|(\\w+)\\s*stock\\s*(\\w+))\", re.IGNORECASE)\n",
        "\n",
        "              availability_pattern = re.compile(r\"(?:stock|stock|availability|in\\s*stock)\", re.IGNORECASE)\n",
        "              match = re.search(availability_pattern, text_content)\n",
        "              # print(match.groups())\n",
        "              if match:\n",
        "                  # print(match.group(0))\n",
        "                  return f\"Availability: {match.group(0).strip()}\"\n",
        "          # If specific keywords not found, return \"Scraped Availability: Unknown\"\n",
        "          return \"Availability: Unknown\"\n",
        "      except Exception as e:\n",
        "          print(f\"Error occurred while scraping: {e}\")\n",
        "          return \"Availability: Unknown\"\n",
        "\n",
        "    def search_google(self,query,num_results):\n",
        "      \"\"\"\n",
        "      Performs a custom search  using the Google Custom Search API.\n",
        "\n",
        "      Args:\n",
        "          query (str): The search query to use.\n",
        "          num_results (int, optional): The number of image search results to return (default: 3).\n",
        "\n",
        "      Returns:\n",
        "          str: A string containing information about the image search results. Each result includes the image URL and its corresponding link.\n",
        "      \"\"\"\n",
        "      # user_query= extract_text_between_hashes(query)\n",
        "      results = self.service.cse().list(\n",
        "          q=query, cx=userdata.get('GOOGLE_CSE_ID'),  # Replace with your custom search engine ID\n",
        "          # searchType=\"image\",  # Specify image search\n",
        "          num=3\n",
        "      ).execute()\n",
        "\n",
        "      search_results = \" \"\n",
        "      for item in results.get(\"items\", []):\n",
        "          # for key, value in item.items():\n",
        "          #   print(\"\\033[32m\" + f\"{key}: {value}\" + \"\\033[0m\")\n",
        "          image_url = item[\"pagemap\"][\"cse_image\"][0][\"src\"]\n",
        "          # print(item[\"pagemap\"][\"metatags\"])\n",
        "          # search_results.append({\"link\":item[\"link\"] ,\n",
        "          #                       \"title\": item[\"title\"] ,\n",
        "          #                       \"snippets\":item[\"snippet\"],\n",
        "          #                       \"image\": image_url,\n",
        "          #                       \"meta\": extract_product_info(item[\"pagemap\"][\"metatags\"])})\n",
        "          search_results += self.extract_product_info(item[\"pagemap\"][\"metatags\"])\n",
        "          search_results += f\"image2: {image_url} \\n\"\n",
        "          search_results += f\"link: {item['link']}\"\n",
        "\n",
        "      # len(search_results)\n",
        "      return helper.clear_code(search_results)\n",
        "\n",
        "\n",
        "    def combined_scraped_api_search(self, query):\n",
        "        # Function to perform a combined search using both scraping and API\n",
        "        # Implement your combined search logic here\n",
        "        self.logger.info(\"Performing combined search for: %s\", query)\n",
        "\n",
        "# Example usage:\n",
        "scraper = WebScraper()\n",
        "query = \"your search query\"\n",
        "\n",
        "scraper.google_scraper(query)\n",
        "scraper.combined_scraped_api_search(query)"
      ],
      "metadata": {
        "id": "EM4eSJq0m6CT",
        "outputId": "4bd685d5-2e5b-4918-b96b-18a00375c890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "2024-03-21 07:40:22,670 - __main__ - INFO - Performing combined search for: your search query\n",
            "INFO:__main__:Performing combined search for: your search query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraper.google_scraper(\"Apotheke 3 wick candles collection\")"
      ],
      "metadata": {
        "id": "qt2HTqzKykFj",
        "outputId": "efc0a5bb-76a7-4519-c22c-235f46f095e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GooglePlease clickhereif you are not redirected within a few seconds.AllImagesShoppingNewsMapsVideosBooksSearch toolsAny timeAny timePast hourPast 24 hoursPast weekPast monthPast yearAll resultsAll resultsVerbatim3-Wick Candles | APOTHEKEapothekeco.com  collections  3-wick-candlesMade with a premium soy wax blend and essential and perfume-grade fragrance oils, our luxury scented 3-wick candles are designed to provide a clean-burning...Canvas 3-Wick Candle - Apothekeapothekeco.com  products  canvas-3-wick-candleRating5.0(8)In stockThis 3-Wick Scented Candle is made with a premium soy wax blend and essential and perfume-grade fragrance oils in a luxe white-translucent glass vessel. The...Charcoal Rouge 3-Wick Candle | APOTHEKEapothekeco.com  products  charcoal-rouge-3-wick-candleRating5.0(2)In stockThis 3-Wick Scented Candle is made with a premium soy wax blend and essential and perfume-grade fragrance oils in a chic matte-black glass vessel. The FragranceAPOTHEKE Market Collection Luxury Scented 3-Wick Jar Candle ...www.amazon.com  APOTHEKE-Market-Collection-Luxury-ScentedRating4.2(147)$72.95Buy APOTHEKE Market Collection Luxury Scented 3-Wick Jar Candle, Meyer Lemon & Mint, 32 oz - Lemon, Spearmint, Jasmine & Eucalyptus Scent, Strong Fragrance,...Shop All Candles | APOTHEKEapothekeco.com  collections  candles-votives-and-3-wicksCandles & Diffusers  Candles  Best Selling Fragrance  Classic Candles  2-Wick Ceramic Candles  3-Wick Candles  Fragrance Bundles  Discovery Sets  Mini...Saffron Vanilla 3-Wick Candle  APOTHEKEapothekeco.com  products  saffron-vanilla-3-wick-candleRating4.8(6)In stockCreate your paradise with the comforting and complex scent of Saffron Vanilla from the Eden Collection. This 3-Wick Scented Candle is made with a premium...Charcoal 3-Wick Candle - Apothekeapothekeco.com  products  charcoal-3-wick-candleRating4.4(125)In stockThis 3-Wick Scented Candle is made with a premium soy wax blend and essential and perfume-grade fragrance oils in a chic matte-black glass vessel. The FragranceApotheke 3-Wick Candles Collection - Amazon.comwww.amazon.com  Scented-Candles-Home-Soy-Wax-Jar-CandleRating4.2(147)$78.00In stockOur candles are hand poured and made with a proprietary blend of essential and perfume-grade oils for a long-lasting fragrance. Each 26 oz candle burns for...APOTHEKE 3-Wick Candle - Goodywww.ongoody.com  apotheke  3-wick-candle-recipient-s-choice-1$78.00Hand-poured with perfume-grade fragrance oils and a high quality soy wax blend. This scent is formulated with a unique wick, wax, and fragrance combination...Apotheke - Tonka Oak 3 Wick Candlewww.candledelirium.com  apotheke-tonka-oak-3-wick-candle$78.00Apotheke Canvas Candle is where crisp linen, white musk, and sweet lily of the valley mingle with a dew drop accord for a clean, refreshing scent.&l.Related searchesApotheke 3 wick candles collection saleApotheke 3 wick candles collection reviewcheapest 3-wick candles bath and body worksApotheke Candle Set3 Wick Candles walmart3 wick candles not in a Jar3-Wick Candle saleAPOTHEKE 4 Wick CandleNext >United StatesFrom your IP address-Learn moreSign inSettingsPrivacyTermsDark theme: Off'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraper.search_google(\"Apotheke 3 wick candles collection price images\",3)"
      ],
      "metadata": {
        "id": "SypGSDOh78Q4",
        "outputId": "9d4eb6d6-2f20-456d-96fe-aea2e811dfb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n ***Title: Apotheke 3-Wick Candles Collection\\nDescription: Captivating Fragrance: Transform any living space with the unforgettable scent of Charcoal 3-Wick Candle from APOTHEKE. Our candles are hand poured and made with a proprietary blend of essential and perfume-grade oils for a long-lasting fragrance. Each 26 oz candle burns for approximately 120 - 1...\\nURL: https://www.amazon.com/dp/B07PMY9PK6/ref=tsm_1_fb_lk \\nAvailability: In Stock\\nASIN: B07PMY9PK6 \\ntype: text \\nimage2: https://m.media-amazon.com/images/I/51PyziKMJdL._AC_UF894,1000_QL80_.jpg \\nlink: https://www.amazon.com/Scented-Candles-Home-Soy-Wax-Jar-Candle/dp/B07PMY9PK6\\n ***Title: 1 Hotels Signature Kindling Candle\\nDescription: An aromatic bouquet of exotic eucalyptus leaves and sultry cedarwood meld together with a transparent Oakwood to create anuplifting impression. While a base of sheer musk and earthy tree mossimpart a touch of natures luxury. Available in multiple sizes: - 2oz Votive -9oz 1 Wick - 28oz 3 Wick\\nPrice: 32.00 USD\\nImage URL: http://shop.1hotels.com/cdn/shop/files/1HCandle1_1200x1200.jpg?v=1682705341 \\nURL: https://shop.1hotels.com/products/1-hotels-signature-kindling-candle \\nAvailability: Unknown\\nSite Name: 1 Hotels Goodthings \\ntype: product \\nimage2: http://shop.1hotels.com/cdn/shop/files/1HCandle1_1200x1200.jpg?v=1682705341 \\nlink: https://shop.1hotels.com/products/1-hotels-signature-kindling-candle\\n ***Title: Apotheke 3-Wick Candles (Cedarwood Ginger, 3-Wick)\\nDescription: Captivating Fragrance: Transform any living space with the unforgettable scent of Cedarwood Ginger 3-Wick Candle from APOTHEKE. Our candles are hand poured and made with a proprietary blend of essential and perfume-grade oils for a long-lasting fragrance. Each 26 oz candle burns for approximately...\\nURL: https://www.amazon.com/dp/B09RTN99Y9/ref=tsm_1_fb_lk \\nAvailability: in stock\\nASIN: B09RTN99Y9 \\ntype: text \\nimage2: https://m.media-amazon.com/images/I/618bD-pCyRL._AC_UF894,1000_QL80_.jpg \\nlink: https://www.amazon.com/Scented-Candles-Home-Soy-Wax-Jar-Candle/dp/B09RTN99Y9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}